{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T21:42:17.078601Z",
     "start_time": "2018-07-06T21:42:17.045836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T21:42:17.243310Z",
     "start_time": "2018-07-06T21:42:17.210356Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T21:42:17.355832Z",
     "start_time": "2018-07-06T21:42:17.325183Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T21:42:17.515490Z",
     "start_time": "2018-07-06T21:42:17.483610Z"
    }
   },
   "outputs": [],
   "source": [
    "from data.frecency import sample, frecency_points\n",
    "from data.frecency import sample_suggestions_normal as sample_suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T21:42:54.405392Z",
     "start_time": "2018-07-06T21:42:54.382187Z"
    }
   },
   "outputs": [],
   "source": [
    "from optimizers import GradientDescent, AdaptiveGradientDescent, DecayedGradientDescent, RProp, Adam\n",
    "from utils import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hinge Loss (SVM loss)\n",
    "\n",
    "To supervise training, we keep logging the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T21:42:17.943701Z",
     "start_time": "2018-07-06T21:42:17.913206Z"
    }
   },
   "outputs": [],
   "source": [
    "def svm_loss(preds, ys, delta=0):\n",
    "    correct = ys.argmax()\n",
    "    score_correct = preds[correct]\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for i, pred in enumerate(preds):\n",
    "        loss += max(0, pred + delta - score_correct)            \n",
    "            \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T21:42:18.079236Z",
     "start_time": "2018-07-06T21:42:18.048513Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_url_features(num_samples):\n",
    "    frequencies = np.int32(np.random.exponential(7, size=num_samples)) + 1\n",
    "    frequencies = np.int32(np.ones(num_samples))\n",
    "    X = []\n",
    "    \n",
    "    for frequency in frequencies:\n",
    "        num_sampled = min(10, frequency)\n",
    "        features = sample_weighted(num_sampled, weights).sum(axis=0)\n",
    "        X.append(frequency / num_sampled * features)\n",
    "        \n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T21:43:25.713947Z",
     "start_time": "2018-07-06T21:43:25.681918Z"
    }
   },
   "outputs": [],
   "source": [
    "def rank_accuracy(y, preds):\n",
    "    correct = 0.\n",
    "    \n",
    "    for yi, pi in zip(y, preds):\n",
    "        if yi[pi.argmax()] == yi.max():\n",
    "            correct += 1\n",
    "            \n",
    "    return correct / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Learning\n",
    "\n",
    "To implement a federated version of the model above, we have to create a `Client` class that completely encapsulates training data. Only the `Client` can compute gradients based on its own data. While the `Server` is the main class for controlling the training process, it can only request gradients from clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T21:42:18.576588Z",
     "start_time": "2018-07-06T21:42:18.553626Z"
    }
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T22:08:40.381160Z",
     "start_time": "2018-07-06T22:08:40.330912Z"
    }
   },
   "outputs": [],
   "source": [
    "class Server:\n",
    "    def __init__(self, clients):\n",
    "        self.clients = clients\n",
    "        \n",
    "        num_features = len(frecency_points)\n",
    "        self.W = np.int32(frecency_points + (np.random.random(size=(num_features)) - 0.5) * 300)\n",
    "        self.W = np.maximum(1, self.W)\n",
    "    \n",
    "    def fit(self, optimizer, num_iterations, num_clients_per_iteration, constraints=[], callbacks=[]):        \n",
    "        update_list = []\n",
    "        W_list = [self.W.copy()]\n",
    "        \n",
    "        for j in range(num_iterations):\n",
    "            clients = random.sample(self.clients, num_clients_per_iteration)\n",
    "            updates, losses = zip(*[client.request_update(self) for client in clients])\n",
    "            update_list.append(updates)\n",
    "            \n",
    "            gradient = np.mean(updates, axis=0)\n",
    "            loss = np.mean(losses, axis=0)\n",
    "            \n",
    "            print(\"[%d/%d] training loss across clients %.5f\" % (j + 1, num_iterations, loss))\n",
    "            \n",
    "            for callback in callbacks:\n",
    "                callback(self)\n",
    "            \n",
    "            self.W += np.int32(optimizer(gradient))\n",
    "            \n",
    "            for constraint in constraints:\n",
    "                self.W = constraint(self.W)\n",
    "                \n",
    "            W_list.append(self.W.copy())\n",
    "                \n",
    "        return update_list, W_list\n",
    "            \n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        \n",
    "        for x in X:\n",
    "            scores = x.dot(self.W)\n",
    "            preds.append(scores)\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T22:08:41.891087Z",
     "start_time": "2018-07-06T22:08:41.870199Z"
    }
   },
   "outputs": [],
   "source": [
    "class FrecencyConstraints:\n",
    "    def __call__(self, gradient):\n",
    "        return gradient - min(0, gradient.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With numerical gradient computation\n",
    "\n",
    "So far all simulations were based on the fact that we knew how to analytically derive the gradient.\n",
    "This is not the case for the actual addon, which uses a simple [finite difference method](https://en.wikipedia.org/wiki/Finite_difference_method) of computing the gradient.\n",
    "To make sure the simulations still work, the following reconstructs the important part of the client-side code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T22:08:42.381835Z",
     "start_time": "2018-07-06T22:08:42.362066Z"
    }
   },
   "outputs": [],
   "source": [
    "def full_loss(model, loss_fn, X, y):\n",
    "    preds = model.predict(X)\n",
    "    return sum([loss_fn(pi, yi) for pi, yi in zip(preds, y)]) / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T22:08:42.564223Z",
     "start_time": "2018-07-06T22:08:42.530200Z"
    }
   },
   "outputs": [],
   "source": [
    "class NumericalClient:\n",
    "    def __init__(self, data_generator, delta=0):\n",
    "        self.data_generator = data_generator\n",
    "        self.delta = 0\n",
    "    \n",
    "    def request_update(self, model, eps=1):\n",
    "        X, y = self.data_generator()\n",
    "        loss = full_loss(model, svm_loss, X, y)\n",
    "        \n",
    "        num_features = X[0].shape[1]\n",
    "        gradient = []\n",
    "        \n",
    "        for i in range(num_features):\n",
    "            model.W[i] -= eps\n",
    "            loss1 = full_loss(model, svm_loss, X, y)\n",
    "            \n",
    "            model.W[i] += 2 * eps\n",
    "            loss2 = full_loss(model, svm_loss, X, y)\n",
    "            \n",
    "            finite_difference = (loss1 - loss2) / (2 * eps)\n",
    "            gradient.append(finite_difference)\n",
    "            \n",
    "            model.W[i] -= eps\n",
    "        \n",
    "        return gradient, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T22:08:42.947203Z",
     "start_time": "2018-07-06T22:08:42.922719Z"
    }
   },
   "outputs": [],
   "source": [
    "clients = [NumericalClient(lambda: sample_suggestions(np.int32(np.random.exponential(.8)) + 1)) for _ in range(5000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T22:15:23.540765Z",
     "start_time": "2018-07-06T22:15:23.518685Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(updates[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T22:09:09.222587Z",
     "start_time": "2018-07-06T22:08:43.606989Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/30] training loss across clients 145.74850\n",
      "[ModelCheckpoint] New best model with 0.55320 validation accuracy\n",
      "[2/30] training loss across clients 140.17196\n",
      "validation: 0.548 accuracy\n",
      "[3/30] training loss across clients 137.06396\n",
      "[ModelCheckpoint] New best model with 0.56320 validation accuracy\n",
      "[4/30] training loss across clients 126.46936\n",
      "[ModelCheckpoint] New best model with 0.58160 validation accuracy\n",
      "[5/30] training loss across clients 100.49808\n",
      "validation: 0.574 accuracy\n",
      "[6/30] training loss across clients 108.09229\n",
      "validation: 0.576 accuracy\n",
      "[7/30] training loss across clients 90.12421\n",
      "validation: 0.567 accuracy\n",
      "[8/30] training loss across clients 93.23899\n",
      "[ModelCheckpoint] New best model with 0.58800 validation accuracy\n",
      "[9/30] training loss across clients 71.37712\n",
      "[ModelCheckpoint] New best model with 0.59080 validation accuracy\n",
      "[10/30] training loss across clients 64.41379\n",
      "validation: 0.586 accuracy\n",
      "[11/30] training loss across clients 68.86979\n",
      "validation: 0.582 accuracy\n",
      "[12/30] training loss across clients 43.58775\n",
      "validation: 0.589 accuracy\n",
      "[13/30] training loss across clients 51.72211\n",
      "validation: 0.589 accuracy\n",
      "[14/30] training loss across clients 49.15121\n",
      "[ModelCheckpoint] New best model with 0.59540 validation accuracy\n",
      "[15/30] training loss across clients 37.79767\n",
      "[ModelCheckpoint] New best model with 0.63900 validation accuracy\n",
      "[16/30] training loss across clients 32.35104\n",
      "[ModelCheckpoint] New best model with 0.65380 validation accuracy\n",
      "[17/30] training loss across clients 30.33425\n",
      "validation: 0.651 accuracy\n",
      "[18/30] training loss across clients 27.03833\n",
      "[ModelCheckpoint] New best model with 0.65620 validation accuracy\n",
      "[19/30] training loss across clients 17.55167\n",
      "[ModelCheckpoint] New best model with 0.77340 validation accuracy\n",
      "[20/30] training loss across clients 18.88833\n",
      "[ModelCheckpoint] New best model with 0.77520 validation accuracy\n",
      "[21/30] training loss across clients 16.53971\n",
      "validation: 0.767 accuracy\n",
      "[22/30] training loss across clients 9.74892\n",
      "validation: 0.768 accuracy\n",
      "[23/30] training loss across clients 10.75033\n",
      "[ModelCheckpoint] New best model with 0.80360 validation accuracy\n",
      "[24/30] training loss across clients 8.58650\n",
      "[ModelCheckpoint] New best model with 0.81740 validation accuracy\n",
      "[25/30] training loss across clients 5.78562\n",
      "validation: 0.808 accuracy\n",
      "[26/30] training loss across clients 5.55926\n",
      "[ModelCheckpoint] New best model with 0.82740 validation accuracy\n",
      "[27/30] training loss across clients 3.01375\n",
      "[ModelCheckpoint] New best model with 0.90060 validation accuracy\n",
      "[28/30] training loss across clients 3.88475\n",
      "[ModelCheckpoint] New best model with 0.91120 validation accuracy\n",
      "[29/30] training loss across clients 1.90446\n",
      "[ModelCheckpoint] New best model with 0.92700 validation accuracy\n",
      "[30/30] training loss across clients 1.02438\n",
      "[ModelCheckpoint] New best model with 0.93660 validation accuracy\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "opt = opt = RProp(2., len(frecency_points), min_value=1, max_value=3, alpha=2., beta=0.6)\n",
    "server = Server(clients)\n",
    "updates, Ws = server.fit(optimizer=opt,\n",
    "          num_iterations=30,\n",
    "           num_clients_per_iteration=400,\n",
    "           constraints=[FrecencyConstraints()],\n",
    "          callbacks=[ModelCheckpoint(rank_accuracy, sample_suggestions, 5000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T22:09:10.729754Z",
     "start_time": "2018-07-06T22:09:09.225375Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(updates)):\n",
    "    np.savetxt(\"updates-%.2d.csv\" % i, updates[i], fmt=\"%.7f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T22:09:10.785024Z",
     "start_time": "2018-07-06T22:09:10.732392Z"
    }
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"weights.csv\", np.int32(Ws), fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "1162px",
    "left": "0px",
    "right": "1494px",
    "top": "160px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
